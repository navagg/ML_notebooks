{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yJK0ECjE5vK"
   },
   "source": [
    "### This Notebook is used to understand Aprioiri Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ivfAbVvYHrRA"
   },
   "source": [
    "**Following Cell is used to load a test dataset containing 5 records. But You Should use your funcion to load the datset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "7AMFPyo964Z8",
    "outputId": "097cea36-bc42-4942-a05d-feae671d5f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[['Bread', 'Milk'],\n",
      " ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
      " ['Milk', 'Diapers', 'Beer', 'Coke'],\n",
      " ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
      " ['Bread', 'Milk', 'Diapers', 'Coke']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "def load_dataset():\n",
    "    \"\"\"Loads an example of market basket transactions for testing purposes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list (database) of lists (transactions). Each element of a transaction \n",
    "    is an item.\n",
    "    \"\"\"\n",
    "    return [['Bread', 'Milk'], \n",
    "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
    "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Coke']]\n",
    "\n",
    "dataset = load_dataset() # list of transactions; each transaction is a list of items\n",
    "D = map(set, dataset) # set of transactions; each transaction is a list of items\n",
    "print(len(dataset))\n",
    "pprint.pprint(dataset)\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g3-Qms981jt9"
   },
   "outputs": [],
   "source": [
    "def create_candidates(dataset, VERBOSE=False):\n",
    "    \"\"\"Creates a list of candidate 1-itemsets from a list of transactions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The list of candidate itemsets (c1) passed as a frozenset (a set that is \n",
    "    immutable and hashable).\n",
    "    \"\"\"\n",
    "    c1 = [] # list of all items in the database of transactions\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "\n",
    "    if VERBOSE:\n",
    "        # Print a list of all the candidate items.\n",
    "        print (\"\" \\\n",
    "            + \"{\" \\\n",
    "            + \"\".join(str(i[0]) + \", \" for i in iter(c1)).rstrip(', ') \\\n",
    "            + \"}\")\n",
    "\n",
    "    # Map c1 to a frozenset because it will be the key of a dictionary.\n",
    "    return map(set, c1)\n",
    "    return(c1)\n",
    "    #return C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FpFZo6K778OX",
    "outputId": "ff612e9b-a318-480c-9f29-e3b4b594ea64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Beer, Bread, Coke, Diapers, Eggs, Milk}\n"
     ]
    }
   ],
   "source": [
    "# Generate candidate itemsets.\n",
    "C1 = create_candidates(dataset, VERBOSE=True) # candidate 1-itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XNCB3kH9J4VP"
   },
   "source": [
    "**Now a function is defined to prune the candidates from the database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "92M3wSFTJ_pm"
   },
   "outputs": [],
   "source": [
    "def support_prune(dataset, candidates, min_support, VERBOSE=True):\n",
    "    \"\"\"Returns all candidate itemsets that meet a minimum support threshold.\n",
    "\n",
    "    By the apriori principle, if an itemset is frequent, then all of its \n",
    "    subsets must also be frequent. As a result, we can perform support-based \n",
    "    pruning to systematically control the exponential growth of candidate \n",
    "    itemsets. Thus, itemsets that do not meet the minimum support level are \n",
    "    pruned from the input list of itemsets (dataset).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate \n",
    "        itemsets.\n",
    "\n",
    "    candidates : frozenset\n",
    "        The list of candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "    \"\"\"\n",
    "    print(\"values inside function\")\n",
    "    pprint.pprint(dataset)\n",
    "    print(\"candidates\")\n",
    "    print(candidates)\n",
    "    sscnt = dict() # set for support counts\n",
    "    num_items=0\n",
    "    #for candidate in iter(candidates):\n",
    "        #print(candidate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for tid in dataset:\n",
    "        print(\"tid\")\n",
    "        print(tid)\n",
    "        num_items+=1\n",
    "        for can in iter(candidates):\n",
    "            print(\"Here1\")\n",
    "            #print(next(can))\n",
    "            if can.issubset(tid):\n",
    "                print(\"Here 2\")\n",
    "                sscnt.setdefault(can,0)\n",
    "                sscnt[can] += 1\n",
    "\n",
    "    #num_items = 5 #len(dataset) # total number of transactions in the dataset\n",
    "    retlist = [] # array for unpruned itemsets\n",
    "    support_data = {} # set for support data for corresponding itemsets\n",
    "    \n",
    "    pprint.pprint(sscnt)\n",
    "    for key in sscnt:\n",
    "        # Calculate the support of itemset key.\n",
    "        print(key)\n",
    "        support = sscnt[key] / num_items\n",
    "        if support >= min_support:\n",
    "            retlist.insert(0, key)\n",
    "        support_data[key] = support\n",
    "\n",
    "    # Print a list of the pruned itemsets.\n",
    "    if VERBOSE:\n",
    "        for kset in retlist:\n",
    "            for item in kset:\n",
    "                print (\"{\" + str(item) + \"}\")\n",
    "        #print\n",
    "        for key in sscnt:\n",
    "            print (\"\" \\\n",
    "                + \"{\" \\\n",
    "                + \"\".join([str(i) + \", \" for i in iter(key)]).rstrip(', ') \\\n",
    "                + \"}\" \\\n",
    "                + \":  sup = \" + str(support_data[key]))\n",
    "\n",
    "    return retlist, support_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yX97iKJQK9TA"
   },
   "source": [
    "**Now Test this Function Support_prune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TIi-Pa-18DVw",
    "outputId": "9d1dbb4d-b54f-4eef-a7a6-d3f21e66c5b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values inside function\n",
      "[['Bread', 'Milk'],\n",
      " ['Bread', 'Diapers', 'Beer', 'Eggs'],\n",
      " ['Milk', 'Diapers', 'Beer', 'Coke'],\n",
      " ['Bread', 'Milk', 'Diapers', 'Beer'],\n",
      " ['Bread', 'Milk', 'Diapers', 'Coke']]\n",
      "candidates\n",
      "<map object at 0x000002304AD25F98>\n",
      "tid\n",
      "['Bread', 'Milk']\n",
      "Here1\n",
      "Here1\n",
      "Here 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'set'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-304b465c7abf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mF1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msupport_prune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVERBOSE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'here 3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msupport_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-d75749d6d29d>\u001b[0m in \u001b[0;36msupport_prune\u001b[1;34m(dataset, candidates, min_support, VERBOSE)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missubset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Here 2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0msscnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcan\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0msscnt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcan\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'set'"
     ]
    }
   ],
   "source": [
    "# Prune candidate 1-itemsets via support-based pruning to generate frequent 1-itemsets.\n",
    "F1, support_data = support_prune(dataset, C1, 0.1, VERBOSE=True)\n",
    "print('here 3')\n",
    "print(F1)\n",
    "print(support_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VsdhM5LPyyw"
   },
   "source": [
    "**Now We will Make two functions 'Apriori' and 'Apriori Gen'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WrAAUmI2KBYs"
   },
   "outputs": [],
   "source": [
    "def apriori(dataset, min_support=0.1, VERBOSE=False):\n",
    "    \"\"\"Implements the Apriori algorithm.\n",
    "\n",
    "    The Apriori algorithm will iteratively generate new candidate \n",
    "    k-itemsets using the frequent (k-1)-itemsets found in the previous \n",
    "    iteration.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : list\n",
    "        The dataset (a list of transactions) from which to generate candidate itemsets.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold. Defaults to 0.5.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    F : list\n",
    "        The list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    C1 = create_candidates(dataset)\n",
    "    D = map(set, dataset)\n",
    "    F1, support_data = support_prune(D, C1, min_support, VERBOSE=False) # prune candidate 1-itemsets\n",
    "    F = [F1] # list of frequent itemsets; initialized to frequent 1-itemsets\n",
    "    k = 2 # the itemset cardinality\n",
    "    while (len(F[k - 2]) > 0):\n",
    "        Ck = apriori_gen(F[k-2], k) # generate candidate itemsets\n",
    "        Fk, supK = support_prune(D, Ck, min_support) # prune candidate itemsets\n",
    "        support_data.update(supK) # update the support counts to reflect pruning\n",
    "        F.append(Fk) # add the pruned candidate itemsets to the list of frequent itemsets\n",
    "        k += 1\n",
    "\n",
    "    if VERBOSE:\n",
    "        # Print a list of all the frequent itemsets.\n",
    "        for kset in F:\n",
    "            for item in kset:\n",
    "                print( \"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join(str(i) + \", \" for i in iter(item)).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  sup = \" + str(round(support_data[item], 3)))\n",
    "\n",
    "    return F, support_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8X3orG5QtX1"
   },
   "outputs": [],
   "source": [
    "def apriori_gen(freq_sets, k):\n",
    "    \"\"\"Generates candidate itemsets (via the F_k-1 x F_k-1 method).\n",
    "\n",
    "    This operation generates new candidate k-itemsets based on the frequent \n",
    "    (k-1)-itemsets found in the previous iteration. The candidate generation \n",
    "    procedure merges a pair of frequent (k-1)-itemsets only if their first k-2 \n",
    "    items are identical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_sets : list\n",
    "        The list of frequent (k-1)-itemsets.\n",
    "\n",
    "    k : integer\n",
    "        The cardinality of the current itemsets begin evaluated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    retlist : list\n",
    "        The list of merged frequent itemsets.\n",
    "    \"\"\"\n",
    "    retList = [] # list of merged frequent itemsets\n",
    "    lenLk = len(freq_sets) # number of frequent itemsets\n",
    "    for i in range(lenLk):\n",
    "        for j in range(i+1, lenLk):\n",
    "            a=list(freq_sets[i])\n",
    "            b=list(freq_sets[j])\n",
    "            a.sort()\n",
    "            b.sort()\n",
    "            F1 = a[:k-2] # first k-2 items of freq_sets[i]\n",
    "            F2 = b[:k-2] # first k-2 items of freq_sets[j]\n",
    "\n",
    "            if F1 == F2: # if the first k-2 items are identical\n",
    "                # Merge the frequent itemsets.\n",
    "                retList.append(freq_sets[i] | freq_sets[j])\n",
    "\n",
    "    return retList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B5uFcJkTTjSr"
   },
   "source": [
    "#### Now Test Run the Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "POdWUhDMTOqZ",
    "outputId": "35d10cba-6ea1-4c49-fc38-83b1638f4b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'Beer'})\n",
      "frozenset({'Bread'})\n",
      "frozenset({'Coke'})\n",
      "frozenset({'Diapers'})\n",
      "frozenset({'Eggs'})\n",
      "frozenset({'Milk'})\n",
      "{'Milk', 'Bread'}\n",
      "{'Eggs', 'Diapers', 'Beer', 'Bread'}\n",
      "{'Coke', 'Milk', 'Diapers', 'Beer'}\n",
      "{'Milk', 'Diapers', 'Beer', 'Bread'}\n",
      "{'Coke', 'Milk', 'Diapers', 'Bread'}\n",
      "{}\n",
      "this is F\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "# Generate all the frequent itemsets using the Apriori algorithm.\n",
    "F, support_data = apriori(dataset, min_support=0.02, VERBOSE=True)\n",
    "print(\"this is F\")\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bZCRcIqCVY-S"
   },
   "source": [
    "Now Functions to generate Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILTy7Kl4Twnj"
   },
   "outputs": [],
   "source": [
    "def rules_from_conseq(freq_set, H, support_data, rules, min_confidence=0.01):\n",
    "    \"\"\"Generates a set of candidate rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A list of frequent itemsets (of a particular length).\n",
    "\n",
    "    support_data : dict\n",
    "        The support data for all candidate itemsets.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "    \"\"\"\n",
    "    m = len(H[0])\n",
    "    if (len(freq_set) > (m+1)):\n",
    "        Hmp1 = apriori_gen(H, m+1) # generate candidate itemsets\n",
    "        Hmp1 = calc_confidence(freq_set, Hmp1,  support_data, rules, min_confidence)\n",
    "        if len(Hmp1) > 1:\n",
    "            # If there are candidate rules above the minimum confidence \n",
    "            # threshold, recurse on the list of these candidate rules.\n",
    "            rules_from_conseq(freq_set, Hmp1, support_data, rules, min_confidence)\n",
    "\n",
    "def calc_confidence(freq_set, H, support_data, rules, min_confidence=0.01, VERBOSE=False):\n",
    "    \"\"\"Evaluates the generated rules.\n",
    "\n",
    "    One measurement for quantifying the goodness of association rules is \n",
    "    confidence. The confidence for a rule 'P implies H' (P -> H) is defined as \n",
    "    the support for P and H divided by the support for P \n",
    "    (support (P|H) / support(P)), where the | symbol denotes the set union \n",
    "    (thus P|H means all the items in set P or in set H).\n",
    "\n",
    "    To calculate the confidence, we iterate through the frequent itemsets and \n",
    "    associated support data. For each frequent itemset, we divide the support \n",
    "    of the itemset by the support of the antecedent (left-hand-side of the \n",
    "    rule).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_set : frozenset\n",
    "        The complete list of frequent itemsets.\n",
    "\n",
    "    H : list\n",
    "        A frequent itemset.\n",
    "\n",
    "    min_support : float\n",
    "        The minimum support threshold.\n",
    "\n",
    "    rules : list\n",
    "        A potentially incomplete set of candidate rules above the minimum \n",
    "        confidence threshold.\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pruned_H : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    pruned_H = [] # list of candidate rules above the minimum confidence threshold\n",
    "    for conseq in H: # iterate over the frequent itemsets\n",
    "        conf = support_data[freq_set] / support_data[freq_set - conseq]\n",
    "        if conf >= min_confidence:\n",
    "\n",
    "            rules.append((freq_set - conseq, conseq, conf))\n",
    "            pruned_H.append(conseq)\n",
    "\n",
    "            if VERBOSE:\n",
    "                print( \"\" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(freq_set-conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \" ---> \" \\\n",
    "                    + \"{\" \\\n",
    "                    + \"\".join([str(i) + \", \" for i in iter(conseq)]).rstrip(', ') \\\n",
    "                    + \"}\" \\\n",
    "                    + \":  conf = \" + str(round(conf, 3)) \\\n",
    "                    + \", sup = \" + str(round(support_data[freq_set], 3)))\n",
    "\n",
    "    return pruned_H\n",
    "\n",
    "def generate_rules(F, support_data, min_confidence, VERBOSE=True):\n",
    "    \"\"\"Generates a set of candidate rules from a list of frequent itemsets.\n",
    "\n",
    "    For each frequent itemset, we calculate the confidence of using a\n",
    "    particular item as the rule consequent (right-hand-side of the rule). By \n",
    "    testing and merging the remaining rules, we recursively create a list of \n",
    "    pruned rules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    F : list\n",
    "        A list of frequent itemsets.\n",
    "\n",
    "    support_data : dict\n",
    "        The corresponding support data for the frequent itemsets (L).\n",
    "\n",
    "    min_confidence : float\n",
    "        The minimum confidence threshold. Defaults to 0.7.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rules : list\n",
    "        The list of candidate rules above the minimum confidence threshold.\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    for i in range(1, len(F)):\n",
    "        for freq_set in F[i]:\n",
    "            H1 = [frozenset([item]) for item in freq_set]\n",
    "            print(H1)\n",
    "            if (i > 1):\n",
    "                rules_from_conseq(freq_set, H1, support_data, rules, min_confidence)\n",
    "            else:\n",
    "                calc_confidence(freq_set, H1, support_data, rules, min_confidence, VERBOSE=VERBOSE)\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_JMaZMCVfhW"
   },
   "source": [
    "Now test the Genration of Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSQNfAJHVejn"
   },
   "outputs": [],
   "source": [
    "# Generate the association rules from a list of frequent itemsets.\n",
    "H = generate_rules(F, support_data, min_confidence=0.001, VERBOSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
