{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "p = np.linspace(0.01, 0.99, 100, endpoint=True)\n",
    "\n",
    "def entropy(p):\n",
    "    q = 1 - p\n",
    "    return -(p*np.log2(p) + q*np.log2(q))\n",
    "\n",
    "h = entropy(p)\n",
    "plt.plot(p, h)\n",
    "plt.title(\"Entropy\")\n",
    "plt.xlabel(\"p1\")\n",
    "plt.ylabel(\"H\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, method=\"simple_tree\", num_trees=None, stop=0.3, \n",
    "    minSize=10, subset=None):\n",
    "        self.method = method\n",
    "        self.num_trees = num_trees\n",
    "        self.stop = stop\n",
    "        self.minSize = minSize\n",
    "        self.subset = subset\n",
    "\n",
    "    def train(self, data, labels):\n",
    "        def simple_tree(data, labels):\n",
    "            '''Method for building a simple decision tree'''\n",
    "            attributes = copy.deepcopy(data.columns.values.tolist())\n",
    "            data['labels'] = pd.Series(labels, index=data.index)\n",
    "            self.classes = data.ix[:, 'labels'].unique()\n",
    "            tree = simple_tree_helper(data, attributes)\n",
    "            return tree\n",
    "\n",
    "        def simple_tree_helper(data, attributes, random=False):\n",
    "            '''Recursive method that builds the decision tree'''\n",
    "            if data.shape[0] > 0 and len(attributes) > 0: \n",
    "                # If all the labels in this group are the same, \n",
    "                # then we are done and return the majority label\n",
    "                if len(data[\"labels\"].unique()) == 1:\n",
    "                    return int(data[\"labels\"].values[0])\n",
    "                # Else if the group entropy is less than p% of the original \n",
    "                elif (calculate_entropy(data[\"labels\"].values) < \n",
    "                    self.stop * math.log(len(self.classes), 2)): \n",
    "                    return majority(data[\"labels\"].values)\n",
    "                elif (data.shape[0] < self.minSize):\n",
    "                    return majority(data[\"labels\"].values)\n",
    "                else:\n",
    "                    # Tree decorrelation step of random forest: random choose \n",
    "                    # a fraction of attributes to choose from\n",
    "                    if random:\n",
    "                        indices = np.random.choice(len(attributes), \n",
    "                            size=math.ceil(self.subset*len(attributes)), replace=False)\n",
    "                        subset = [attributes[i] for i in indices]\n",
    "                        bestAttribute, splitVal = choose_attribute(data, subset)\n",
    "                    else:\n",
    "                        bestAttribute, splitVal = choose_attribute(data, attributes)\n",
    "                    attributes.remove(bestAttribute)\n",
    "                    if isinstance(splitVal, list):\n",
    "                        left = data[data[bestAttribute].isin(splitVal[0])]\n",
    "                        right = data[data[bestAttribute].isin(splitVal[1])]\n",
    "                    else:\n",
    "                        left = data[data[bestAttribute] <= splitVal]\n",
    "                        right = data[data[bestAttribute] > splitVal]\n",
    "                    tree = BinaryTree((bestAttribute, splitVal))\n",
    "                    if len(left) == 0 or len(right) == 0:\n",
    "                        return majority(data[\"labels\"].values)\n",
    "                    tree.insertLeft(simple_tree_helper(left, attributes))\n",
    "                    tree.insertRight(simple_tree_helper(right, attributes))\n",
    "                    return tree\n",
    "            elif data.shape[0] > 0 and len(attributes) == 0:\n",
    "                return majority(data[\"labels\"].values)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        def random_forest(data, labels):\n",
    "            '''Method for building a random forest'''\n",
    "            trees = []\n",
    "            attributes = copy.deepcopy(data.columns.values.tolist())\n",
    "            attributesTemp = copy.deepcopy(attributes)\n",
    "            data['labels'] = pd.Series(labels, index=data.index)\n",
    "            self.classes = data.ix[:, 'labels'].unique()\n",
    "            for i in range(self.num_trees):\n",
    "                # Bagging: randomly picking 2/3 of the data for making a simple \n",
    "                # decision tree\n",
    "                baggingIdx = np.random.choice(data.shape[0], size=data.shape[0], \n",
    "                    replace=True)\n",
    "                bag = data.iloc[baggingIdx, :]\n",
    "                tree = simple_tree_helper(bag, attributesTemp, True)\n",
    "                trees.append(tree)\n",
    "                attributesTemp = copy.deepcopy(attributes)\n",
    "            return trees\n",
    "\n",
    "        # @ input: data frame, available attributes\n",
    "        # @ output: attribute and attribute value to split on that maximizes difference \n",
    "        # between parent entropy and \n",
    "        #   average children entropy\n",
    "        def choose_attribute(data, attributes):\n",
    "            bestGain = 0\n",
    "            bestAttribute = None\n",
    "            bestSplit = None\n",
    "            parentEntropy = calculate_entropy(data.ix[:, \"labels\"].values)\n",
    "            for attribute in attributes:\n",
    "                if len(data[attribute].unique()) == 1:\n",
    "                    gain = 0\n",
    "                    bestAttribute = attribute\n",
    "                    bestSplit = data[attribute].values[0]\n",
    "                else:\n",
    "                    gain, splitVal = calculate_gain(data.ix[:, [attribute, \"labels\"]], \n",
    "                        attribute, parentEntropy)\n",
    "                    if gain >= bestGain:\n",
    "                        bestGain = gain\n",
    "                        bestSplit = splitVal\n",
    "                        bestAttribute = attribute\n",
    "            return bestAttribute, bestSplit\n",
    "\n",
    "        # @ input: data frame, selected attribute, parent entropy\n",
    "        # @ output: best entropy gain and best split for that attribute\n",
    "        def calculate_gain(data, attribute, parentEntropy):\n",
    "            total = data.shape[0]\n",
    "            minEntropy = float(\"inf\")\n",
    "            splitVal = None\n",
    "            bestLeft = None\n",
    "            bestRight = None\n",
    "            # If attribute is categorical, sort attribute value by class proportion for \n",
    "            # that attribute value, and choose best split point going from least class \n",
    "            # proportion to greatest class proportion\n",
    "            if type(data[attribute].values[0]) == str: \n",
    "                label = self.classes[0]\n",
    "                n = len(data[data[\"labels\"] == label])\n",
    "                attributeVals = data[attribute].unique()\n",
    "                proportions = [(val, data[(data[attribute] == val) & \n",
    "                    (data[\"labels\"] == label)].shape[0] / n) for val in attributeVals]\n",
    "                proportions = sorted(proportions, key=lambda x: x[1])\n",
    "                if len(proportions) == 1:\n",
    "                    leftLabels = data[\"labels\"].values\n",
    "                    minEntropy = calculate_entropy(leftLabels)\n",
    "                    splitVal = [[proportions[0][0]], []]\n",
    "                else: \n",
    "                    for i in range(1, len(proportions)):\n",
    "                        leftVal = [tup[0] for tup in proportions[:i]]\n",
    "                        rightVal = [tup[0] for tup in proportions[i:len(proportions)]]\n",
    "                        leftLabels = data[data[attribute].isin(leftVal)][\"labels\"].values\n",
    "                        rightLabels = data[data[attribute].isin(rightVal)][\"labels\"].values\n",
    "                        leftEntropy = calculate_entropy(leftLabels)\n",
    "                        rightEntropy = calculate_entropy(rightLabels)\n",
    "                        avgEntropy = ((len(leftLabels) / total) * leftEntropy \n",
    "                            + (len(rightLabels) / total) * rightEntropy)\n",
    "                        if avgEntropy < minEntropy:\n",
    "                            minEntropy = avgEntropy\n",
    "                            splitVal = [leftVal, rightVal]\n",
    "                            bestLeft = leftEntropy\n",
    "                            bestRight = rightEntropy\n",
    "            # If attribute is continuous, sort attribute value, then choose best split \n",
    "            # point from least to greatest only from points corresponding to class \n",
    "            # label change.\n",
    "            else: \n",
    "                data = data.sort_values(attribute, axis=0)\n",
    "                diff = data[\"labels\"].diff(1).values\n",
    "                idx = np.where(diff == -1)[0] - 1\n",
    "                if len(idx) > 50:\n",
    "                    step = len(idx) // 50\n",
    "                    idxindex = list(range(0, len(idx) + step, step))\n",
    "                    idxindex = idxindex[:len(idxindex) - 1]\n",
    "                    idx = idx[idxindex]\n",
    "                if len(idx) == 0:\n",
    "                    minEntropy = calculate_entropy(data[\"labels\"].values)\n",
    "                    splitVal = data[attribute].values[0]\n",
    "                else: \n",
    "                    attributeVals = data.iloc[idx, :][attribute].unique()\n",
    "                    if len(attributeVals) > 1:\n",
    "                        end = len(attributeVals) - 1\n",
    "                    else:\n",
    "                        end = 1\n",
    "                    for val in attributeVals[:end]:\n",
    "                        leftLabels = data[data[attribute] <= val][\"labels\"].values\n",
    "                        rightLabels = data[data[attribute] > val][\"labels\"].values\n",
    "                        leftEntropy = calculate_entropy(leftLabels)\n",
    "                        rightEntropy = calculate_entropy(rightLabels)\n",
    "                        if rightEntropy == None:\n",
    "                            avgEntropy = leftEntropy\n",
    "                        else:\n",
    "                            avgEntropy = ((len(leftLabels) / total) * leftEntropy + \n",
    "                                (len(rightLabels) / total) * rightEntropy)\n",
    "                        if avgEntropy < minEntropy:\n",
    "                            minEntropy = avgEntropy\n",
    "                            splitVal = val\n",
    "            return parentEntropy - minEntropy, splitVal\n",
    "\n",
    "        def calculate_entropy(data):\n",
    "            '''Calculate entropy of given list of class labels'''\n",
    "            if not isinstance(data, np.ndarray):\n",
    "                data = np.array(data)\n",
    "            n = len(data)\n",
    "            if n == 0:\n",
    "                return None\n",
    "            entropy = 0.0\n",
    "            for label in self.classes:\n",
    "                subset = np.where(data == label)[0]\n",
    "                p = len(subset)/n + 1e-6\n",
    "                entropy -= p * math.log(p, 2)\n",
    "            return entropy\n",
    "\n",
    "        def majority(labels):\n",
    "            '''Returns the majority label'''\n",
    "            a = Counter(labels)\n",
    "            vote = a.most_common(1)[0][0]\n",
    "            return int(vote)\n",
    "\n",
    "        class BinaryTree:\n",
    "            def __init__(self, decision):\n",
    "                self.decision = decision\n",
    "                self.leftChild = None\n",
    "                self.rightChild = None\n",
    "            def insertLeft(self, newNode):\n",
    "                self.leftChild = newNode\n",
    "            def insertRight(self, newNode):\n",
    "                self.rightChild = newNode\n",
    "            def getRightChild(self):\n",
    "                return self.rightChild\n",
    "            def getLeftChild(self):\n",
    "                return self.leftChild\n",
    "            def setNode(self, decision):\n",
    "                self.decision = decision\n",
    "            def getNode(self):\n",
    "                return self.decision\n",
    "\n",
    "        if self.method == \"random_forest\": \n",
    "            trees = random_forest(data, labels)\n",
    "            self.tree = trees\n",
    "        elif self.method == \"simple_tree\": \n",
    "            tree = simple_tree(data, labels)\n",
    "            self.tree = tree\n",
    "    def predict(self, dataset):\n",
    "        def transverse_classify(tree, sample):\n",
    "            if isinstance(tree, int):\n",
    "                return int(tree)\n",
    "            else:\n",
    "                decision = tree.getNode()\n",
    "                attribute = decision[0]\n",
    "                splitVal = decision[1]\n",
    "                sampleVal = sample[attribute]\n",
    "                if isinstance(splitVal, list):\n",
    "                    if sampleVal in splitVal[0]:\n",
    "                        tree = tree.getLeftChild()\n",
    "                    else:\n",
    "                        tree = tree.getRightChild()\n",
    "                else:\n",
    "                    if sampleVal <= splitVal:\n",
    "                        tree = tree.getLeftChild()\n",
    "                    else:\n",
    "                        tree = tree.getRightChild()\n",
    "                return transverse_classify(tree, sample)\n",
    "\n",
    "        def majority(labels):\n",
    "            '''Returns the majority label'''\n",
    "            a = Counter(labels)\n",
    "            vote = a.most_common(1)[0][0]\n",
    "            return int(vote)\n",
    "\n",
    "        tree = self.tree\n",
    "        predictions = []\n",
    "        if self.method == \"simple_tree\":\n",
    "            for rownum in range(dataset.shape[0]):\n",
    "                predict = transverse_classify(tree, dataset.iloc[rownum, :])\n",
    "                predictions.append(predict)\n",
    "        # In random forest, the majority vote among all trees is the prediction\n",
    "        elif self.method == \"random_forest\":\n",
    "            for rownum in range(dataset.shape[0]):\n",
    "                ballot = []\n",
    "                for t in tree:\n",
    "                    predict = transverse_classify(t, dataset.iloc[rownum, :])\n",
    "                    ballot.append(predict)\n",
    "                vote = majority(ballot)\n",
    "                predictions.append(vote)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(data, attributes):\n",
    "    for attribute in attributes:\n",
    "        if \"?\" in data[attribute].unique().tolist():\n",
    "            mode = majority(data[attribute].values)\n",
    "            indices = data[data[attribute] == \"?\"].index.tolist()\n",
    "            data.ix[indices, attribute] = mode\n",
    "    return data\n",
    "\n",
    "def majority(labels):\n",
    "    '''Returns the majority label'''\n",
    "    a = Counter(labels)\n",
    "    vote = a.most_common(1)[0][0]\n",
    "    return vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def evaluate_accuracy(predictions, validationLabel):\n",
    "    n = len(predictions)\n",
    "    return sum(predictions == validationLabel) / n\n",
    "\n",
    "data = pd.read_csv(\"datasets/data.csv\")\n",
    "attributes = data.columns.values\n",
    "data = fill_missing(data, attributes)\n",
    "n = data.shape[0]\n",
    "randomIdx = np.random.permutation(n)\n",
    "data = data.iloc[randomIdx, :]\n",
    "trainData = data.iloc[:9*n//10, :]\n",
    "validationData = data.iloc[9*n//10:n, :]\n",
    "trainLabel = trainData.ix[:, \"label\"].values\n",
    "validationLabel = validationData.ix[:, \"label\"].values\n",
    "trainData2 = copy.deepcopy(trainData)\n",
    "validationData2 = copy.deepcopy(validationData)\n",
    "trainLabel2 = copy.deepcopy(trainLabel)\n",
    "validationLabel2 = copy.deepcopy(validationLabel)\n",
    "del trainData[\"label\"]\n",
    "del validationData[\"label\"]\n",
    "print(\"Simple decision tree\")\n",
    "classifier = DecisionTree()\n",
    "classifier.train(trainData, trainLabel)\n",
    "predictions = np.array(classifier.predict(validationData))\n",
    "print(\"Accuracy: \", evaluate_accuracy(predictions, validationLabel))\n",
    "print(\"Random Forest\")\n",
    "classifier = DecisionTree(method=\"random_forest\", num_trees=200, subset=0.1)\n",
    "classifier.train(trainData2, trainLabel2)\n",
    "predictions = np.array(classifier.predict(validationData2))\n",
    "print(\"Accuracy: \", evaluate_accuracy(predictions, validationLabel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "p = np.linspace(0.01, 0.99, 100, endpoint=True)\n",
    "\n",
    "def entropy(p):\n",
    "    q = 1 - p\n",
    "    return (1 - p**2 - q**2)\n",
    "\n",
    "gini = entropy(p)\n",
    "plt.plot(p, gini)\n",
    "plt.title(\"Gini Index\")\n",
    "plt.xlabel(\"p1\")\n",
    "plt.ylabel(\"Gini Index\")\n",
    "plt.ylim(0, 1.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
